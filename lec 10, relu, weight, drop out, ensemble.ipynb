{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4eff263f4434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Epoch: {:>4}] cost = {:>.9}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Learning Finished!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "# relu\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import torch.nn.init\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu,\n",
    "                            linear2, relu,\n",
    "                            linear3, relu,\n",
    "                            linear4, relu,\n",
    "                            linear5)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\vitam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-983a6ce655ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Epoch: {:>4}] cost = {:>.9}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Learning Finished!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "#drop out\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import torch.nn.init\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "# p is the probability of being dropped in PyTorch\n",
    "dropout = torch.nn.Dropout(p=1 - keep_prob)\n",
    "\n",
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, dropout,\n",
    "                            linear2, relu, dropout,\n",
    "                            linear3, relu, dropout,\n",
    "                            linear4, relu, dropout,\n",
    "                            linear5)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/b6/38d9ab1b16c456224823e737f1bb95fe3ff056f3834fba01cd157d59b574/mxnet-1.4.0.post0-py2.py3-none-win_amd64.whl (21.9MB)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Collecting numpy<1.15.0,>=1.8.2 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/fe/8d2eee6d18281415904ef40a12bd7e11162d5304943dbceda4b4e2d50f33/numpy-1.14.6-cp37-none-win_amd64.whl (13.4MB)\n",
      "Collecting requests<2.19.0,>=2.18.4 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
      "Collecting idna<2.7,>=2.5 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB)\n",
      "Collecting urllib3<1.23,>=1.21.1 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitam\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\vitam\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Installing collected packages: graphviz, numpy, idna, urllib3, requests, mxnet\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\vitam\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-azmywrix\\\\users\\\\vitam\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\multiarray.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-41ab61aedf75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# set the seeds. However, this does not guarantee that the result will always be the same since CUDNN is non-deterministic\n",
    "np.random.seed(777)\n",
    "mx.random.seed(77)\n",
    "random.seed(7777)\n",
    "\n",
    "# 1. Loading MNIST\n",
    "mnist = fetch_mldata(dataname='MNIST original')\n",
    "X, y = mnist.data, mnist.target\n",
    "X = X.astype(np.float32) / 255.0\n",
    "X_train, X_valid, X_test = X[:55000].reshape((-1, 1, 28, 28)),\\\n",
    "    X[55000:60000].reshape((-1, 1, 28, 28)),\\\n",
    "    X[60000:].reshape((-1, 1, 28, 28))\n",
    "y_train, y_valid, y_test = y[:55000], y[55000:60000], y[60000:]\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "num_models = 2\n",
    "\n",
    "\n",
    "def build_symbol():\n",
    "    data = mx.sym.var(name=\"data\")\n",
    "    label = mx.sym.var(name=\"label\")\n",
    "\n",
    "    L1 = mx.sym.Convolution(data=data, kernel=(3, 3), pad=(1, 1), num_filter=32, name='L1_conv')\n",
    "    L1 = mx.sym.Activation(data=L1, act_type='relu', name='L1_relu')\n",
    "    L1 = mx.sym.Pooling(data=L1, kernel=(2, 2), stride=(2, 2), pool_type='max', name='L1_pool')\n",
    "    L1 = mx.sym.Dropout(L1, p=0.3, name=\"L1_dropout\")\n",
    "\n",
    "    L2 = mx.sym.Convolution(data=L1, kernel=(3, 3), pad=(1, 1), num_filter=64, name='L2_conv')\n",
    "    L2 = mx.sym.Activation(data=L2, act_type='relu', name='L2_relu')\n",
    "    L2 = mx.sym.Pooling(data=L2, kernel=(2, 2), stride=(2, 2), pool_type='max', name='L2_pool')\n",
    "    L2 = mx.sym.Dropout(L2, p=0.3, name=\"L2_dropout\")\n",
    "\n",
    "    L3 = mx.sym.Convolution(data=L2, kernel=(3, 3), pad=(1, 1), num_filter=128, name='L3_conv')\n",
    "    L3 = mx.sym.Activation(data=L3, act_type='relu', name='L3_relu')\n",
    "    L3 = mx.sym.Pooling(data=L3, kernel=(2, 2), stride=(2, 2), pad=(1, 1), pool_type='max', name='L3_pool')\n",
    "    L3 = mx.sym.flatten(L3)\n",
    "    L3 = mx.sym.Dropout(L3, p=0.3, name=\"L3_dropout\")\n",
    "\n",
    "    L4 = mx.sym.FullyConnected(data=L3, num_hidden=625, name='L4_fc')\n",
    "    L4 = mx.sym.Dropout(L4, p=0.5)\n",
    "\n",
    "    logits = mx.sym.FullyConnected(data=L4, num_hidden=10, name='logits')\n",
    "\n",
    "    loss = mx.sym.mean(-mx.sym.pick(mx.sym.log_softmax(logits), label, axis=-1))\n",
    "    loss = mx.sym.make_loss(loss)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "def get_batch(p, batch_size, X, y):\n",
    "    data_npy = np.take(X,\n",
    "                       indices=np.arange(p * batch_size, (p + 1) * batch_size),\n",
    "                       axis=0,\n",
    "                       mode=\"clip\")\n",
    "    label_npy = np.take(y,\n",
    "                        indices=np.arange(p * batch_size, (p + 1) * batch_size),\n",
    "                        axis=0,\n",
    "                        mode=\"clip\")\n",
    "    num_valid = batch_size if (p + 1) * batch_size <= X.shape[0] else X.shape[0] - p * batch_size\n",
    "    return data_npy, label_npy, num_valid\n",
    "\n",
    "\n",
    "train_nets = []\n",
    "test_nets = []\n",
    "# 1. Get the symbol\n",
    "loss, logits = build_symbol()\n",
    "\n",
    "# 2. Build the training nets and testing nets\n",
    "data_desc = mx.io.DataDesc(name='data', shape=(batch_size, 1, 28, 28), layout='NCHW')\n",
    "label_desc = mx.io.DataDesc(name='label', shape=(batch_size, ), layout='N')\n",
    "for i in range(num_models):\n",
    "    net = mx.mod.Module(symbol=loss,\n",
    "                        data_names=[data_desc.name],\n",
    "                        label_names=[label_desc.name],\n",
    "                        context=mx.gpu())\n",
    "    net.bind(data_shapes=[data_desc], label_shapes=[label_desc])\n",
    "    net.init_params(initializer=mx.init.Xavier())\n",
    "    net.init_optimizer(optimizer=\"adam\",\n",
    "                       optimizer_params={'learning_rate': learning_rate,\n",
    "                                         'rescale_grad': 1.0},\n",
    "                       kvstore=None)\n",
    "\n",
    "    # We build another testing network that outputs the logits.\n",
    "    test_net = mx.mod.Module(symbol=logits,\n",
    "                             data_names=[data_desc.name],\n",
    "                             label_names=None,\n",
    "                             context=mx.gpu())\n",
    "    # Setting the `shared_module` to ensure that the test network shares the same parameters and\n",
    "    #  allocated memory of the training network\n",
    "    test_net.bind(data_shapes=[data_desc],\n",
    "                  label_shapes=None,\n",
    "                  for_training=False,\n",
    "                  grad_req='null',\n",
    "                  shared_module=net)\n",
    "    train_nets.append(net)\n",
    "    test_nets.append(test_net)\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# 3. Train all the models\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(num_models)\n",
    "    total_batch = int(math.ceil(X_train.shape[0] / batch_size))\n",
    "    shuffle_ind = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "    X_train = X_train[shuffle_ind, :]\n",
    "    y_train = y_train[shuffle_ind]\n",
    "    for i in range(total_batch):\n",
    "        data_npy, label_npy, _ = get_batch(i, batch_size, X_train, y_train)\n",
    "        for i, net in enumerate(train_nets):\n",
    "            net.forward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n",
    "                                                   label=[nd.array(label_npy)]),\n",
    "                        is_train=True)\n",
    "            loss_nd = net.get_outputs()[0]\n",
    "            net.backward()\n",
    "            net.update()\n",
    "            avg_cost_list[i] += loss_nd.asnumpy()[0] / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# 5. Test the networks\n",
    "total_batch = int(np.ceil(X_test.shape[0] / batch_size))\n",
    "correct_counts = [0 for i in range(num_models)]\n",
    "ensemble_correct_count = 0\n",
    "total_num = 0\n",
    "for i in range(total_batch):\n",
    "    num_valid = batch_size if (i + 1) * batch_size <= X_test.shape[0]\\\n",
    "        else X_test.shape[0] - i * batch_size\n",
    "    data_npy, label_npy, num_valid = get_batch(i, batch_size, X_test, y_test)\n",
    "    prob_ensemble = nd.zeros(shape=(label_npy.shape[0], 10), ctx=mx.gpu())\n",
    "    for i, test_net in enumerate(test_nets):\n",
    "        test_net.forward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n",
    "                                                    label=None),\n",
    "                         is_train=False)\n",
    "        logits_nd = test_net.get_outputs()[0]\n",
    "        prob_nd = nd.softmax(logits_nd)\n",
    "        prob_ensemble += prob_nd\n",
    "        pred_cls = nd.argmax(prob_nd, axis=-1).asnumpy()\n",
    "        correct_counts[i] += (pred_cls[:num_valid] == label_npy[:num_valid]).sum()\n",
    "    prob_ensemble /= num_models\n",
    "    ensemble_pred_cls = nd.argmax(prob_ensemble, axis=-1).asnumpy()\n",
    "    ensemble_correct_count += (ensemble_pred_cls[:num_valid] == label_npy[:num_valid]).sum()\n",
    "for i in range(num_models):\n",
    "    print(i, 'Accuracy:', correct_counts[i] / float(X_test.shape[0]))\n",
    "print('Ensemble accuracy:', ensemble_correct_count / float(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
